#
# This file lists features that we decided not to implement yet.
# Many of these ideas are actually good, in particular the "normal"
# category, and we may revive them in the future.  When in doubt, we
# prefer to not support a feature, as adding a feature in the future can
# always be done without breaking compatibility, whereas removing one is
# always backward-incompatible.
#

#==== Optimizations ====

#
# When parsing rules, only save the "body" part of the rule (everything except
# the target) as an unparsed string.  Parse it properly only when needed.  This
# will not catch certain errors anymore.
#

#
# Dep hierarchy:  instead of using shared_ptr<> and
# dynamic_pointer_cast<>, roll our own code, storing the type inside
# Dep.
#

#
# Cache the content of files that are used as dynamic dependencies or
# dynamic variables.
#

#
# Use C instead of C++.  This is tempting for simplicity and also out of
# principle.  What would be necessary:
#  - Write container classes indexed with strings and targets.
#  - Convert certain containers to arrays/linked lists.
#  - More complex and error-prone memory management, in particular for
#    strings
#  - A replacement for exceptions
#  - For Dep objects, keep track of the type in the FLAGS, and
#    have our own pointer conversion functions.  Also, have our own
#    reference counting.
#
# Remember:  Even GCC is written in C++ nowadays, so we are not in bad
# company.
#
# If we then use 'c99', we could claim full POSIX compliance.
#

#==== Normal features ====

#
# Have an option to output the dependency graph, i.e., output each edge
# on a single line, e.g. separated by tab.  (Use it then to perform a network
# analysis of the KONECT-Analysis dependency graph.)
#

#
# Line number and file syntax:  A preprocessor-like construct that gives a new
# filename and line number, such that error messages from Stu can point
# to the original file, not the generated file.  Analogous to the
# preprocessors #line directive.  Useful in dynamic dependencies.
#

% file datasets.list
% line 120

# Use this in stu-utils/texdep and other stu-utils programs judiciously.

#
# In -j>1 mode, Start trivial dependencies immediately when it is clear
# they must be started, not just when everything else is done.
#

#
# When including files using dynamic dependencies, make a
# difference between inclusion and import, as is done for %include and
# %import.
#

A: [-d data/B];

#
# Search within predefined paths for %include ($STU_PATH or $STUPATH;
# the -I option)
#

# This is included from the "Stu path", and could be installed in
# e.g. /usr/share/stu/lib/ or ~/share/stu/lib/, etc.
% include c++.stu

#
# Have a way to set $STU_SHELL and $STU_CP from within a Stu script.
# E.g., using directives.
#

# Some Stu scripts may want to use Bash throughout
% STU_SHELL /bin/bash

# Should it be valid for the whole Stu invocation, or only for the
# source file in which it appears?

# Could also be done as
% set STU_SHELL /bin/bash

#
# A flag with the meaning "never rebuild that dependency", but only use
# it to determine whether we need to rebuilt the target.  It would mean
# that we can prune dependencies with that flag as soon as we know that
# the target must be built anyway.
#

# In this example, when 'program' does not exist, Stu will *not* build
# 'dep.sources', because the -s flag is used and Stu knows that
# 'program' has to be rebuilt anyway.  '-s' could stand for
# "secondary", and mean "we don't want this to be built, we only need it
# to determine whether we have to be rebuilt."

program:  -s [dep.sources] {
	cc ... -o program
}
dep.sources {
	# Some complex mechanism for determining the source code files
	# of the program
}

#
# -m bfs (breadth-first order) and -m target (pseudorandom by target
# name, i.e., same pseudorandom order for each target, as long as its
# dependencies are the same).
#

#
# A 'why' option that shows why things are built.  Will look similar to
# error traces, only containing explanations.  In essence, output a line
# everytime one of the 'need_build' variables is updated.  Also, we need
# to do something smart with timestamps.
#

# * 'abc' does not exist
# * 'abc' is older than its dependency 'xyz'
# * ...

#
# Have an extended safety mode, in which Stu checks that none of the
# dependencies themselves where touched by a command.  How deep should
# this go?  Should it also check dependency included dynamically?
#

#
# Have a signal to stop Stu, but without interrupting currently running
# jobs.  I.e., let all currently running jobs finish, and then quit
# (with appropriate exit status).
#

#
# Have an option for: For hardcoded rules, compare the content of the

list.${X:.}${Y:.} {
	compute -x $X -y $Y >list.${X}${Y}
}

# When the date is saved as contiguous digits, e.g. 'log-analysis.20180413.txt'
# for April 13, 2018, but as ISO dat in 'log-*.txt'.
log-analysis.${year:[0-9]{4}}${month:[0-9]{2}}${day:[0-9]{2}}.txt:
	log.$year-$month-$dat.txt
{ ... }

# The parameter $NETWORK must not contain a dot
web.${NETWORK:[^.]+} :  ... ;

# Specify a list of values
${NAME:ref|ukob|tuberlin}.bib:  bibs-A/${NAME}.bib
{ cp bibs-A/$NAME.bib $NAME.bib }
$NAME.bib:  bibs-B/$NAME.bib
{ cp bibs-B/$NAME.bib $NAME.bib }

# Exclude slashes
${NAME:[^/]+}.bib: ...;

#
# Fingerprinting instead of timestamps is a feature that is implemented
# by quite a few Make replacements.  This could also be combined with
# preprocessors which ignore whitespace, etc.  Can also be used for the
# commands embedded in Stu scripts, but then again we don't want to
# rebuild everything just because a command changed.  Another downside
# is that it needs Stu to go through every file in full, making Stu's
# runtime be linear in the file size.  At the moment, the runtime of Stu
# is independent of file size, but of course the command to generate a
# file is always linear in the file size.  As many features in this
# file, this opens a enormous can of worms.  Another way to implement it
# is to compute a fingerprint when before executing the command, and
# then again after the command succeeds, and when the result is
# identical, then reset the timestamp of the file back to its old
# value.  (This could be a script for stu-utils/.)
#

#
# Let Stu itself be multithreaded.  Very complex to implement, and has
# only very little benefit since Stu already starts many processes.
# Would only make sense in scenarios with thousands of processes, which
# would only make sense on machines with thousands of cores.  But still
# very sensible and maybe necessary one day, in principle.
#

#
# A Stu server.  The current setup of Stu (and also Make) means that
# when nothing is to be done, Stu still has to walk the complete
# dependency tree and stat() every file.  For large projects, this may
# become very slow.  Instead, have a Stu daemon which uses inotify or
# similar to notice changes.  There are many practical issues with that,
# so that would be a huge change.  Would almost likely mean the server
# itself would do the building, and the client would only communicate
# with the server.  This also means the server would need to be
# multi-threaded.  This could also be used to build a Stu GUI (either
# local or via HTTP) that allows one to control a running Stu server.
#
# May actually be trivial to implement once we have proper job
# control.  But not for now.
#

#
# The ability to declare additional command interpreters.  Note:  we may
# use another character than the backslash.
#
# A common theme among Make replacements is to hardcode specific,
# high-level, programming languages.  This makes certain applications
# very nice and elegant, namely those that use only features found in
# these languages.  The idea for Stu is to allow to use any programming
# language in commands, with a syntax that makes it easy to add new
# command interpreters.  What we don't want to have are hardcoded
# programming languages.
#
# There are many details to think about for this, and in general this
# type of feature pushes people to want to hardcode their favorite
# programming language in Stu, which we don't want.
#
# Here's a random example using Julia, assuming that "everything is a
# matrix".
#

A.data:  B.data C.data
\julia{
	% This command will be executed by the Julia wrapper, and
	% read/write the three matrices from the corresponding files
	% automatically.
	A = B * C;
}

\julia:  julia-command {
	# This will be executed whenever A.data is built, and the three
	# variables used below will be filled by Stu with the dependencies,
	# the target and the command, which are worked on by the script
	# ./julia-command to invoke Julia accordingly.
	./julia-command "$STU_DEPENDENCIES" "$STU_TARGET" "$STU_COMMAND"
}

# A shell script that converts command to correct Julia scripts.  E.g.,
# it will load B.data and C.data as the variables B and C, and will save
# the generated variable A into A.data.
julia-command;

# Interpreters can be chained
\julia-or-octave:  \julia;

# This may get problematic with finding the end of a command when
# programming languages use unusual quoting mechanisms and ways of
# pairing parenthesis-like characters.

# An alternative is to use '#!', as in the following example:
A.data:  B.data C.data julia-wrapper
{#! julia-wrapper
	A = B * C
}

#
# Nested rules.  Good for dependencies that are used only once.  This is
# quite hard to read, honestly, and mostly fulfills the fantasy of
# people who like deeply nested constructs :)  But implementation-wise,
# it's not that difficult.
#

A: (B: C { cp C B ; }) (D: E {cp E D ; }) { cat B D >A ; }

#
# Alternative dependencies.  Would not be used often, and where it would
# be needed, it can be implemented using dynamic dependency.  On the
# other hand, the '|' operator is free...
#

A:  B | C { ... }
A:  (B C) | (D E) { ... }

#
# Transient targets can have content.  This will be executed at every Stu
# invocation.  This is similar to the way variables worked in Stu 0.
#
# This might be used like variables in Make, but the result will be that
# targets depending on them will always be rebuilt.  It would thus only
# make sense together with fingerprinting.
#
# No very useful, since files can be used in exactly the same way, and
# without the problem of not remembering what was already done.
#

# Their definition:
@NAME = { linux }
@NAME2 = @NAME;
>@NAME { ./printname }

# Their use:
file:  <@NAME { sed -e '...' >file }
result.eps:  $[@NAME] { ./compute --name "$NAME" }

#
# An option of Stu that generates a shell script from a Stu script that
# executes the content.  This can be done to any degree of fidelity to
# the Stu specification, i.e., always build everything, or actually
# check timestamps.
#

#
# libstu:  have the functionality available as a library for C, or for
# any other programming language.  I would rather have a Stu server and
# any programming language can then communicate with the Stu server.
#

#
# Input and output to transients as a way to implement pipe-like
# constructs.
#

>@data.filtered:  <data
{
	sed -e '...'
}

plot.eps:  <@data.filtered
{
	... -o plot.eps
}

# We could also have Stu use pipes using current syntax, in order to
# execute two "connected" rules.
A:  <B { ./command-A --output A ; }
>B:    { ./command-B ; }
# Both commands may be executed in parallel by Stu, using pipes and
# tee-like behaviour

#==== External features ====

#
# These features can be implemented outside of Stu using constructs
# supported by Stu.  Thus, there is no need to integrate them into Stu,
# and in fact supporting them would bloat Stu and unnecessary
# dependencies to it.
#
# If anything, the question we should ask for each of these features is
# not whether we should hardcode it into Stu, but how easily these
# things can be implemented using the Stu language.  If necessary, we
# may then extend the Stu language to make such implementations
# possible, and easy.
#

#
# Things that can be implemented by simply using the appropriate
# commands:
#
# * Compression, archiving, and encryption
# * Converting encodings, line endings, and processing escape sequences
# * Limitations on runtime, memory usages and other resources by jobs
# * Calling interpreters or compilers for specific programming languages
#

#
# Things that can be implemented by rules in Stu:
#
# * Rules for configuring and compiling code
#

#
# Run jobs in a distributed fashion, i.e., on different hosts.  The idea
# here would be to use any software for distributing jobs and call it
# for each Stu job.  This would slightly interfere with what Stu itself
# is doing, since a distributed job management system also has to decide
# when a job is executed.  Most also have dependency management
# capabilities.
#
